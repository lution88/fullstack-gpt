{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 document chain에 대해 알아보자.\n",
    "\n",
    "지금은 off-the-shelf chain들을 사용할거다.\n",
    "\n",
    "그 다음에 우리만의 chain을 LangChain Expression Language으로 만들거다.\n",
    "\n",
    "지금은 LCEL이 대세!!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다양한 type의 document chain 생성방식 중 먼저 우리가 사용해 볼 것은 'stuff'이다.\n",
    "\n",
    "우리가 찾은 document들로 prompt를 stuff(채우기)하는데 사용한다는 뜻이다.\n",
    "\n",
    "1. 먼저 질문을 한다. \"what is foo?\"\n",
    "1. 그 질문을 사용해서 document를 search(검색)할 거다.\n",
    "1. 그 다음 찾은 document들을 Prompt에 입력해서 model에게 전달한다.\n",
    "1. 그리고 model은 입력된 질문과 documents를 토대로 우리에게 답변해줄 거다.\n",
    "\n",
    "여기엔 이미 우리를 위해 만들어져 있는 (off-the-shelf) chain이 있다.\n",
    "\n",
    "---\n",
    "\n",
    "RetrievalQA를 불러오자.\n",
    "\n",
    "일단 출력 결과를 한번 보고나서 직접 코드를 작성해서 구현해보자.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "chain = RetrievalQA.from_chain_type()\n",
    "```\n",
    "이건 일종의 Constructor(생성자함수)인데, LLM을 입력값으로 받는다.\n",
    "\n",
    "chain_type 도 입력가능하다. default='stuff'\n",
    "\n",
    "그리고 RetrievalQA chain은 retriever도 입력값으로 요구한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "retriever는 무엇일까?\n",
    "\n",
    "문서에 따르면 retriever는 class의 interface이다.\n",
    "\n",
    "document를 많은 장소로부터 Retrieve(선별하여 가져오기) 할 수 있다.\n",
    "\n",
    "vector store 말고 다른데서도!\n",
    "\n",
    "이번에는 vector store를 가져와서 Retriever로 만들어보자.\n",
    "\n",
    "```python\n",
    "retriever=vectorstore.as_retriever(),\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# llm\n",
    "llm = ChatOpenAI()\n",
    "# 캐시 경로\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.pdf\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# .from_bytes_store 메서드는 임베딩 작업을 위해 필요한 embedder의 입력을 요구한다.\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings,\n",
    "    cache_dir,\n",
    ")\n",
    "\n",
    "vectorstore = Chroma.from_documents(docs, embeddings)\n",
    "\n",
    "# chain 구현\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 이 chain을 실행(run)해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(\"Where does Winston Smith live?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(\"Describe Victory Mansions?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chroma가 아닌 다른 vectorstores인 FAISS 를 사용해보자.\n",
    "\n",
    "한 줄만 변경해도 적용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# llm\n",
    "llm = ChatOpenAI()\n",
    "# 캐시 경로\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.pdf\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# .from_bytes_store 메서드는 임베딩 작업을 위해 필요한 embedder의 입력을 요구한다.\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings,\n",
    "    cache_dir,\n",
    ")\n",
    "\n",
    "# vectorstore 변경\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# chain 구현: chain_type 변경\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    ")\n",
    "\n",
    "chain.run(\"Describe Victory Mansions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 이번 섹션에서 했던 첫 번째 작업은 바로 file들을 load하는 거였다.\n",
    "\n",
    "text_loader를 이용해서 file들을 load하는 방법을 배움.\n",
    "\n",
    "그리고 UnstructuredFileLoader에 대해서 살펴봤다.\n",
    "\n",
    "UnstructuredFileLoader로 다양한 file들을 load할 수 있다.\n",
    "\n",
    "그 다음 file을 분할(split)하는 방법도 배웠다. 아주 긴 text를 다루기 위해서\n",
    "\n",
    "또 그 긴 text를 작은 document들로 나누고자 했다.\n",
    "\n",
    "거대한 단일 document보다는 작은 여러개를 LLM에게 전달할 때 검색 성능이 더 좋아지기 떄문에.\n",
    "\n",
    "작업을 요약하면, document를 적재(load)하고 분할(split)했다.\n",
    "\n",
    "---\n",
    "\n",
    "그리고 또 embedding에 대해 배웠다.\n",
    "\n",
    "임베딩은 text에 의미별로 적절한 점수를 부여해서 vector 형식으로 표현했다.\n",
    "\n",
    "우리는 OpenAIEmbeddings model을 사용했다.\n",
    "\n",
    "그리고 CacheBacedEmbedding을 사용하여 만들어진 Embedding을 cache(저장)했다.\n",
    "\n",
    "CacheBacedEmbedding에 embeddings model을 전달하고, 데이터가 cache될 directory(폴더)를 지정했다.\n",
    "\n",
    "그 다음으로는 Vector store를 호출했다. 마지막에는 FAISS\n",
    "\n",
    "document와 embedding와 함께 .from_documents 메서드를 호출했다.\n",
    "\n",
    "이 메서드는 document별로 embedding 작업 후 결과를 저장한 vector store를 반환하는데\n",
    "\n",
    "이를 이용해 document 검색도 하고, 연관성이 높은 document들을 찾기도 했다.\n",
    "\n",
    "---\n",
    "\n",
    "마지막으로 RetrievalQA라는 Chain.\n",
    "\n",
    "필요한 입력값은 llm, chain의 type, 그리고 retriever.\n",
    "\n",
    "\n",
    "```python\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    ")\n",
    "\n",
    "```\n",
    "retriever는 Langchain이 제공하는 class 또는 interface의 일종인데,\n",
    "\n",
    "document를 검색해서 찾아오는(retrieve)기능을 가지고 있다.\n",
    "\n",
    "as_retriever() 메서드 호출만으로 vector store 을 retriever로 전환할 수 있었다.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
