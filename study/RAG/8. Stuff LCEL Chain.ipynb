{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain Expression Language 를 사용해서 stuff chain을 구현해보자.\n",
    "\n",
    "랭체인 구성요소\n",
    "\n",
    "- Prompt\n",
    "- Retriever\n",
    "- LLM, ChatModel\n",
    "- Tool\n",
    "- OutputParser\n",
    "\n",
    "우린 이미 Prompt, ChatModel, OutputParser에 대해 알아봤다.\n",
    "\n",
    "Retriever에 대해 알아보자.\n",
    "\n",
    "Retriever는 한 개의 string을 입력받는다.\n",
    "\n",
    "질문이나 그와 관련성이 있는 document를 얻기위한 query를 입력받는다.\n",
    "\n",
    "그리고 Retriever의 출력값은 document들의 List 이다.\n",
    "\n",
    "입출력 타입을 안다면 구현(implement)은 쉽다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 구성 요소들의 순서를 생각해보자.\n",
    "\n",
    "먼저 모든 document를 가져와야 한다. query와 관련있는 건 전부!\n",
    "\n",
    "그렇다면 chain에 Retriever를 첫 번째로 넣어주면 되겠다.\n",
    "\n",
    "retriver는 document들의 list를 반환한다.\n",
    "\n",
    "그 document들은 template에 입력되어야 한다.\n",
    "\n",
    "ChatPromptTemplate를 사용해서 chat messages를 사용하는 template를 만들어보자.\n",
    "\n",
    "```python\n",
    "# template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"you are a helpful assistant. answer question using only the following context. if you don't know the answer just say you don't know, don't make it up\\n\\n{context}\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "그 다음 구성요소는 prompt와 그 내부에 입력될 데이터를 전달받을 거야.\n",
    "\n",
    "바로 LLM(ChatOpenAI).\n",
    "\n",
    "```python\n",
    "chain = retriever | prompt | llm\n",
    "```\n",
    "\n",
    "이제 chain을 실행해야 하는데, run이 아니라 invoke 메서드를 사용한다.\n",
    "\n",
    "```python\n",
    "chain.invoke(\"Describe Victory Mansions\")\n",
    "```\n",
    "\n",
    "이걸 실행하면, 이 string(\"Describe Victory Mansions\")이 retriever에게 전달될 것이다.\n",
    "\n",
    "retriever 는 document들의 list를 반환할 거고\n",
    "\n",
    "그 document들은 context값으로 입력되어야 하고\n",
    "\n",
    "질문(\"Describe Victory Mansions\")도 prompt template의 question으로 입력되어야 한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# retriver는 document들의 list를 반환한다.\u001b[39;00m\n\u001b[0;32m     47\u001b[0m chain \u001b[38;5;241m=\u001b[39m retriever \u001b[38;5;241m|\u001b[39m prompt \u001b[38;5;241m|\u001b[39m llm\n\u001b[1;32m---> 49\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDescribe Victory Mansions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gsbeo\\Documents\\fullstack-gpt\\env\\Lib\\site-packages\\langchain\\schema\\runnable\\base.py:1213\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   1211\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1212\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[1;32m-> 1213\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1214\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1215\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   1216\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1217\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1218\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   1221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\gsbeo\\Documents\\fullstack-gpt\\env\\Lib\\site-packages\\langchain\\schema\\prompt_template.py:60\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Dict, config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     59\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_variables\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gsbeo\\Documents\\fullstack-gpt\\env\\Lib\\site-packages\\langchain\\schema\\runnable\\base.py:715\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m    708\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    709\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    710\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    711\u001b[0m     run_type\u001b[38;5;241m=\u001b[39mrun_type,\n\u001b[0;32m    712\u001b[0m     name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    713\u001b[0m )\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 715\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    719\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\gsbeo\\Documents\\fullstack-gpt\\env\\Lib\\site-packages\\langchain\\schema\\runnable\\config.py:308\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    307\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gsbeo\\Documents\\fullstack-gpt\\env\\Lib\\site-packages\\langchain\\schema\\prompt_template.py:62\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke.<locals>.<lambda>\u001b[1;34m(inner_input)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Dict, config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     59\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_prompt(\n\u001b[1;32m---> 62\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43m{\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_variables\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     63\u001b[0m         ),\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m     65\u001b[0m         config,\n\u001b[0;32m     66\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     67\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\gsbeo\\Documents\\fullstack-gpt\\env\\Lib\\site-packages\\langchain\\schema\\prompt_template.py:62\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Dict, config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     59\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_prompt(\n\u001b[1;32m---> 62\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{key: \u001b[43minner_input\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_variables}\n\u001b[0;32m     63\u001b[0m         ),\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m     65\u001b[0m         config,\n\u001b[0;32m     66\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     67\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# llm\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "# 캐시 경로\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.pdf\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# .from_bytes_store 메서드는 임베딩 작업을 위해 필요한 embedder의 입력을 요구한다.\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings,\n",
    "    cache_dir,\n",
    ")\n",
    "\n",
    "# vectorstore 변경\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# retriever 구현\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            you are a helpful assistant. \n",
    "            answer question using only the following context. \n",
    "            if you don't know the answer just say you don't know. \n",
    "            don't make it up \\n\\n{context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# retriver는 document들의 list를 반환한다.\n",
    "chain = retriever | prompt | llm\n",
    "\n",
    "chain.invoke(\"Describe Victory Mansions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "당장 실행해 보면 코드가 작동하지 않는데, 그 이유는 우리가 아직 구체적인 동작을 구현하지 않았기 때문이다.\n",
    "\n",
    "```python\n",
    "chain = retriever | prompt | llm\n",
    "\n",
    "chain.invoke(\"Describe Victory Mansions\")\n",
    "```\n",
    "\n",
    "위에서 설명한 동작을 실제로 구현해봐야 한다.\n",
    "\n",
    "우리가 원하는 동작은 질문값을 retriever에 전달하고 그 결과로 출력된 document들의 list를 prompt의 context에 입력하고\n",
    "\n",
    "또 이 질문을 prompt의 question property(human의 question)로 전달하는 것 까지이다.\n",
    "\n",
    "이 동작을 구현하기위해, 앞서 해본것과 굉장히 비슷한 작업을 할 거다.\n",
    "\n",
    "{}를 사용해서 prompt의 context는 retriever로부터 오도록 한다.\n",
    "\n",
    "```python\n",
    "chain = {\"context\": retriever} | prompt | llm\n",
    "```\n",
    "\n",
    "전에 본 것처럼 retriever는 invoke에 입력해준 질문을 입력받아 호출될 것이다.\n",
    "\n",
    "그리고 이 질문이 prompt의 question property로 전달되로록 해줘야 한다.\n",
    "\n",
    "이건 RunnablePassthrough를 사용해서 아주 쉽게 할 수 있다.\n",
    "\n",
    "```python\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm\n",
    "```\n",
    "\n",
    "RunnablePassthrough는 말 그대로 입력값이 통과하게(pass through) 해준다.\n",
    "\n",
    "그러니까 질문(입력값)이 RunnablePassthrough()를 통해서 RunnablePassthrough()이 위치한 모든 곳들에 전달되도록 해준다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Victory Mansions is a building where Winston Smith resides. It is described as having glass doors, a hallway that smells of boiled cabbage and old rag mats, and being seven flights up. The building is depicted as having a faulty lift, with a large colored poster of a man\\'s face with a caption that reads \"BIG BROTHER IS WATCHING YOU\" displayed indoors. The building is part of the cityscape of London, which is portrayed as a grim and colorless environment with rotting houses and oppressive government propaganda posters.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# llm\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "# 캐시 경로\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.pdf\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# .from_bytes_store 메서드는 임베딩 작업을 위해 필요한 embedder의 입력을 요구한다.\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings,\n",
    "    cache_dir,\n",
    ")\n",
    "\n",
    "# vectorstore 변경\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# retriever 구현\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"you are a helpful assistant. answer question using only the following context. if you don't know the answer just say you don't know, don't make it up\\n\\n{context}\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# retriver는 document들의 list를 반환한다.\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "chain.invoke(\"Describe Victory Mansions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실행해 보면 RunnablePassthrough() 덕분에 잘 동작하는 걸 확인할 수 있다.\n",
    "\n",
    "Retriever는 chain의 아주 중요한 구성요소라는 것을 기억하자.\n",
    "\n",
    "우린 Retriever를 chain의 구성요소로 사용할 수 있고,\n",
    "\n",
    "Retriever는 한 개의 string을 입력받고 document들의 list를 출력해준다.\n",
    "\n",
    "위에서 한 것은\n",
    "\n",
    "```python\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "```\n",
    "\n",
    "LangChain이 질문(\"Describe Victory Mansions\")을 retriever의 입력값으로 전달해서 호출해줬다.\n",
    "\n",
    "```python\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever(\"Describe Victory Mansions\"),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "```\n",
    "\n",
    "그럼 document들의 list가 반환되는 거다.\n",
    "\n",
    "```python\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": [Doc],\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "```\n",
    "\n",
    "그리고 RunnablePassthrough는 이 입력을 prompt의 question에 전달해준다.\n",
    "\n",
    "```python\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever(\"Describe Victory Mansions\"),\n",
    "        \"question\": \"Describe Victory Mansions\",\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그럼 document들의 list, 그리고 question, 이 두 가지가 prompt에 전달될 거다.\n",
    "\n",
    "여기서 format_messages메서드를 호출하는 것과 같은 동작이 발생한다.\n",
    "\n",
    "context값으로 document들이 들어가고, question도 옆에 입력될 것이다.\n",
    "\n",
    "```python\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever(\"Describe Victory Mansions\"),\n",
    "        \"question\": \"Describe Victory Mansions\",\n",
    "    }\n",
    "    | prompt.format_messages(context=context, qeustion=question,)\n",
    "    | llm\n",
    ")\n",
    "```\n",
    "\n",
    "그 다음으론 format된 prompt를 LLM에게 전달한다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
