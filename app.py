import os
import time

import streamlit as st
from langchain.callbacks.base import BaseCallbackHandler
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import UnstructuredFileLoader
from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.storage import LocalFileStore
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from openai import OpenAIError


class ChatCallbackHandler(BaseCallbackHandler):
    message = ""

    def on_llm_start(self, *args, **kwargs):
        self.message_box = st.empty()

    def on_llm_end(self, *args, **kwargs):
        save_message(self.message, "ai")

    def on_llm_new_token(self, token, *args, **kwargs):
        self.message += token
        self.message_box.markdown(self.message)


def folder_exist(folder_path):
    if not os.path.exists(folder_path):
        os.makedirs(folder_path)


def validate_api_key(api_key):
    """API í‚¤ì˜ ìœ íš¨ì„±ì„ ê²€ì‚¬í•©ë‹ˆë‹¤."""
    try:
        # í…ŒìŠ¤íŠ¸ìš© LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        test_llm = ChatOpenAI(temperature=0.1, openai_api_key=api_key)
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ë¡œ API í‚¤ ê²€ì¦
        test_llm.predict("í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ì…ë‹ˆë‹¤.")
        return True, None
    except OpenAIError as e:
        return False, str(e)
    except Exception as e:
        return False, f"ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


def load_openai_api_key():
    """Streamlit ì‚¬ì´ë“œë°”ì—ì„œ OpenAI API í‚¤ë¥¼ ë¡œë“œí•˜ê³  ì„¤ì •í•©ë‹ˆë‹¤."""
    if "api_key_configured" not in st.session_state:
        st.session_state.api_key_configured = False

    if "api_key_error" not in st.session_state:
        st.session_state.api_key_error = None

    with st.sidebar:
        st.markdown("## OpenAI API ì„¤ì •")
        api_key = st.text_input(
            "OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
            type="password",
            help="OpenAI API í‚¤ëŠ” ì•ˆì „í•˜ê²Œ ì €ì¥ë˜ë©° ì„¸ì…˜ ë‚´ì—ì„œë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.",
            key="openai_api_key",
        )

        if api_key:
            # API í‚¤ ìœ íš¨ì„± ê²€ì‚¬
            is_valid, error_message = validate_api_key(api_key)

            if is_valid:
                os.environ["OPENAI_API_KEY"] = api_key
                st.session_state.api_key_configured = True
                st.session_state.api_key_error = None
                st.success("âœ… API í‚¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤!")
                return True
            else:
                st.session_state.api_key_configured = False
                st.session_state.api_key_error = error_message
                st.error(f"âš ï¸ API í‚¤ ì˜¤ë¥˜: {error_message}")
                return False
        else:
            st.session_state.api_key_configured = False
            st.warning("âš ï¸ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return False


def handle_chat_error(func):
    """ì±„íŒ… ê´€ë ¨ í•¨ìˆ˜ì˜ ì˜ˆì™¸ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°ì½”ë ˆì´í„°"""

    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except OpenAIError as e:
            st.error(f"OpenAI API ì˜¤ë¥˜: {str(e)}")
            # API í‚¤ ìƒíƒœ ì´ˆê¸°í™”
            st.session_state.api_key_configured = False
            st.warning("API í‚¤ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

    return wrapper


@handle_chat_error
def process_message(message, chain):
    """ë©”ì‹œì§€ ì²˜ë¦¬ ë° ì‘ë‹µ ìƒì„±"""
    with st.chat_message("ai"):
        chain.invoke(message)


st.set_page_config(
    page_title="QuizGPT",
    page_icon="ğŸ¤–",
)

if "messages" not in st.session_state:
    st.session_state["messages"] = []


@st.cache_data(show_spinner="Embedding file...")
def embed_file(file):
    try:
        file_content = file.read()
        file_folder = "./.cache/files"
        folder_exist(file_folder)
        file_path = f"{file_folder}/{file.name}"
        with open(file_path, "wb") as f:
            f.write(file_content)
        cache_folder = "./.cache/embeddings"
        folder_exist(cache_folder)
        cache_dir = LocalFileStore(f"{cache_folder}/{file.name}")
        splitter = CharacterTextSplitter.from_tiktoken_encoder(
            separator="\n",
            chunk_size=600,
            chunk_overlap=100,
        )
        loader = UnstructuredFileLoader(file_path)
        docs = loader.load_and_split(text_splitter=splitter)
        embeddings = OpenAIEmbeddings()
        cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
            embeddings, cache_dir
        )
        vectorstore = FAISS.from_documents(docs, cached_embeddings)
        retriever = vectorstore.as_retriever()
        return retriever
    except OpenAIError as e:
        st.error(f"ì„ë² ë”© ìƒì„± ì¤‘ OpenAI API ì˜¤ë¥˜: {str(e)}")
        return None
    except Exception as e:
        st.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None


def save_message(message, role):
    st.session_state["messages"].append({"message": message, "role": role})


def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)


def paint_history():
    for message in st.session_state["messages"]:
        send_message(message["message"], message["role"], save=False)


def format_docs(docs):
    return "\n\n".join(document.page_content for document in docs)


prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
        Answer the question using ONLY the following context.
        If you don't know the answer just say you don't know.
        Don't make anything up.

        Context: {context}
        """,
        ),
        ("human", "{question}"),
    ]
)

st.title("Quiz GPT")

st.markdown(
    """
Welcome!!!

Use this chatbot to ask questions to an AI about your files!

Upload your files on the sidebar.
"""
)

# API í‚¤ í™•ì¸
is_api_key_configured = load_openai_api_key()

if is_api_key_configured:
    try:
        # API í‚¤ê°€ ì„¤ì •ëœ ê²½ìš°ì—ë§Œ LLM ì´ˆê¸°í™”
        llm = ChatOpenAI(
            temperature=0.1,
            streaming=True,
            callbacks=[ChatCallbackHandler()],
        )

        with st.sidebar:
            file = st.file_uploader(
                "Upload a .txt .pdf or .docx file", type=["pdf", "txt", "docx"]
            )

        if file:
            retriever = embed_file(file)
            if retriever is not None:  # ì„ë² ë”© ì„±ê³µí•œ ê²½ìš°ì—ë§Œ ì§„í–‰
                send_message("I'm ready! Ask away!", "ai", save=False)
                paint_history()
                message = st.chat_input("Ask anything about your file..")
                if message:
                    send_message(message, "human")
                    chain = (
                        {
                            "context": retriever | RunnableLambda(format_docs),
                            "question": RunnablePassthrough(),
                        }
                        | prompt
                        | llm
                    )
                    process_message(message, chain)
        else:
            st.session_state["messages"] = []

    except OpenAIError as e:
        st.error(f"OpenAI API ì˜¤ë¥˜: {str(e)}")
        st.session_state.api_key_configured = False
        st.warning("API í‚¤ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")
    except Exception as e:
        st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
else:
    st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ OpenAI API í‚¤ë¥¼ ë¨¼ì € ì„¤ì •í•´ì£¼ì„¸ìš”.")
